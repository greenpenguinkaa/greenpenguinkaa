{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Навигация по уроку**\n",
        "1. [Как создать нейро-сотрудника?](https://colab.research.google.com/drive/1lEGaSx8gJyKHzrqnptLv7JA9w85fo8fT)\n",
        "2. [Практическая реализация нейро-сотрудника](https://colab.research.google.com/drive/1g9YfsQFqrKCLgz8POS0Cnhtt-d2-JCLm)\n",
        "3. Домашняя работа"
      ],
      "metadata": {
        "id": "BRSQ6SEVe5vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai gradio tiktoken langchain langchain-openai langchain-community chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4svo2JCe8vY",
        "outputId": "5885d140-bf77-4b3e-8e1b-e3b7b2308b5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.43.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.10)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.16.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.16)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting requests>=2.26.0 (from tiktoken)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass # для работы с паролями\n",
        "import os      # для работы с окружением и файловой системой\n",
        "\n",
        "# Запрос ввода ключа от OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите Open API Key:\")"
      ],
      "metadata": {
        "id": "RAWTegHdfAkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "              {\n",
        "                \"doc\": \"https://docs.google.com/document/d/1QTLpBju_WWY_Q25DWcfHvKAWLMMlNyMc3Q9D992azC4/edit?usp=sharing\",\n",
        "                \"prompt\": '''Ты менеджер клуба по игре в покер не на деньги, к тебе могут обращаться hr менеджеры и рекрутеры за подсказками и ответами на их вопросы в чате компании.\n",
        "                        Постарайся дать развернутый ответ, твоя задача ответить так, чтобы у менеджера не осталось больше вопросов к тебе.\n",
        "                        Отвечай по существу, без лишних эмоций и слов, от тебя нужна только точная информация.\n",
        "                        Отвечай максимально точно по документу, не придумывай ничего от себя.\n",
        "                        Документ с информацией для ответа клиенту: ''',\n",
        "                \"name\": \"Нейро-менеджер поддержки HR-отдела\",\n",
        "                \"query\": \"Для чего нужны шаблоны вакансий?\"\n",
        "              }\n",
        "            ]"
      ],
      "metadata": {
        "id": "2q_lEPdGfC4i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Блок библиотек фреймворка LangChain\n",
        "\n",
        "# Работа с документами в langchain\n",
        "from langchain.docstore.document import Document\n",
        "# Эмбеддинги для OpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# Доступ к векторной базе данных\n",
        "from langchain.vectorstores import Chroma\n",
        "# Разделение текста на куски или чанки (chunk)\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Отправка запросов\n",
        "import requests\n",
        "\n",
        "#Доступ к OpenAI\n",
        "from openai import OpenAI\n",
        "\n",
        "# Отприсовка интерфейса с помощью grad\n",
        "import gradio as gr\n",
        "\n",
        "# Библиотека подсчёта токенов\n",
        "# Без запроcов к OpenAI, тем самым не тратим деньги на запросы\n",
        "import tiktoken\n",
        "\n",
        "# Для работы с регулярными выражениями\n",
        "import re"
      ],
      "metadata": {
        "id": "PX1hywFVfGpv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "1d0a22f7-fb28-4982-858c-0e89b1b12555"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "Module langchain_community.embeddings not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/_api/module_import.py\u001b[0m in \u001b[0;36mimport_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3657738074.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Эмбеддинги для OpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Доступ к векторной базе данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/embeddings/openai.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Look up attributes dynamically.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_import_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/_api/module_import.py\u001b[0m in \u001b[0;36mimport_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0;34m\"You can install it using `pip install -U langchain-community`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     )\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: Module langchain_community.embeddings not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Объявляем класс нейро-сотрудника\n",
        "class GPT():\n",
        "    # Объявляем конструктор класса, для передачи имени модели и инициализации атрибутов класса\n",
        "    def __init__(self, model=\"gpt-3.5-turbo\"):\n",
        "        self.log = ''               # атрибут для сбора логов (сообщений)\n",
        "        self.model = model          # атрибут для хранения выбранной модели OpenAI\n",
        "        self.search_index = None    # атрибут для хранения ссылки на базу знаний (если None, то модель не обучена)\n",
        "        self.client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]) # при инициализации запрашиваем ключ от OpenAI\n",
        "\n",
        "    # Метод загрузки текстового документа в векторную базу знаний\n",
        "    def load_search_indexes(self, url):\n",
        "        # Извлекаем document ID гугл документа из URL с помощью регулярных выражений\n",
        "        match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "\n",
        "        # Если ID не найден - генерируем исключение\n",
        "        if match_ is None:\n",
        "            raise ValueError('Неверный Google Docs URL')\n",
        "\n",
        "        # Первый элемент в результате поиска\n",
        "        doc_id = match_.group(1)\n",
        "\n",
        "        # Скачиваем гугл документ по его ID в текстовом формате\n",
        "        response = requests.get(f'https://docs.google.com/document/d/{doc_Poker.txt}/export?format=txt')\n",
        "\n",
        "        # При неудачных статусах запроса будет вызвано исключение\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Извлекаем данные как текст\n",
        "        text = response.text\n",
        "\n",
        "        # Вызываем метод векторизации текста и сохранения в векторную базу данных\n",
        "        return self.create_embedding(text)\n",
        "\n",
        "    # Подсчет числа токенов в строке по имени модели\n",
        "    def num_tokens_from_string(self, string):\n",
        "            \"\"\"Возвращает число токенов в строке\"\"\"\n",
        "            encoding = tiktoken.encoding_for_model(self.model)  # получаем кодировщик по имени модели\n",
        "            num_tokens = len(encoding.encode(string))           # расчитываем строку с помощью кодировщика\n",
        "            return num_tokens                                   # возвращаем число токенов\n",
        "\n",
        "    # Метод разбора текста и его сохранение в векторную базу знаний\n",
        "    def create_embedding(self, data):\n",
        "        # Список документов, полученных из фрагментов текста\n",
        "        source_chunks = []\n",
        "        # Разделяем текст на строки по \\n (перенос на новую строку) или длине фрагмента (chunk_size=1024) с помощью сплитера\n",
        "        # chunk_overlap=0 - означает, что фрагменты не перекрываются друг с другом.\n",
        "        # Если больше нуля, то захватываем дополнительное число символов от соседних чанков.\n",
        "        splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1024, chunk_overlap=0)\n",
        "\n",
        "        # Применяем splitter (функцию расщепления) к данным и перебираем все получившиеся чанки (фрагменты)\n",
        "        for chunk in splitter.split_text(data):\n",
        "            # LangChain работает с документами, поэтому из текстовых чанков мы создаем фрагменты документов\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={}))\n",
        "\n",
        "        # Подсчет числа токенов в документах без запроса к OpenAI (экономим денежные средства)\n",
        "        count_token = self.num_tokens_from_string(' '.join([x.page_content for x in source_chunks]))\n",
        "        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        self.log += f'Количество токенов в документе : {count_token}\\n'\n",
        "\n",
        "        # Создание индексов документа. Применяем к нашему списку документов эмбеддингов OpenAi и в таком виде загружаем в базу ChromaDB\n",
        "        self.search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(), )\n",
        "        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        self.log += f'Данные из документа загружены в в векторную базу данных\\n'\n",
        "\n",
        "        # Возвращаем ссылку на базу данных\n",
        "        return self.search_index\n",
        "\n",
        "    # Демонстрация более аккуратного расчета числа токенов в зависимости от модели\n",
        "    def num_tokens_from_messages(self, messages, model):\n",
        "        \"\"\"Возвращает число токенов из списка сообщений\"\"\"\n",
        "        try:\n",
        "            encoding = tiktoken.encoding_for_model(model) # получаем кодировщик по имени модели\n",
        "        except KeyError:\n",
        "            print(\"Предупреждение: модель не создана. Используйте cl100k_base кодировку.\")\n",
        "            encoding = tiktoken.get_encoding(\"cl100k_base\") # если по имени не нашли, то используем базовый для моделей OpenAI\n",
        "        # Выбор модели\n",
        "        if model in {\n",
        "            \"gpt-3.5-turbo-0613\",\n",
        "            \"gpt-3.5-turbo-16k-0613\",\n",
        "            \"gpt-4-0314\",\n",
        "            \"gpt-4-32k-0314\",\n",
        "            \"gpt-4-0613\",\n",
        "            \"gpt-4-32k-0613\",\n",
        "            \"gpt-4o\",\n",
        "            \"gpt-4o-2024-05-13\"\n",
        "            }:\n",
        "            tokens_per_message = 3 # дополнительное число токенов на сообщение\n",
        "            tokens_per_name = 1    # токенов на имя\n",
        "        elif model == \"gpt-3.5-turbo-0301\":\n",
        "            tokens_per_message = 4  # каждое сообщение содержит <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            tokens_per_name = -1  # если есть имя, то роль не указывается\n",
        "        elif \"gpt-3.5-turbo\" in model:\n",
        "            self.log += f'Внимание! gpt-3.5-turbo может обновиться в любой момент. Используйте gpt-3.5-turbo-0613. \\n'\n",
        "            return self.num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "        elif \"gpt-4\" in model:\n",
        "            self.log += f'Внимание! gpt-4 может обновиться в любой момент. Используйте gpt-4-0613. \\n'\n",
        "            return self.num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "        else: # исключение, если модель не поддерживается\n",
        "            raise NotImplementedError(\n",
        "                f\"\"\"num_tokens_from_messages() не реализован для модели {model}.\"\"\"\n",
        "            )\n",
        "\n",
        "        # Запускаем подсчет токенов\n",
        "        num_tokens = 0                        # счетчик токенов\n",
        "        for message in messages:              # цикл по всем сообщениям\n",
        "            num_tokens += tokens_per_message  # прибовляем число токенов на каждое сообщение\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value)) # считаем токены в сообщении с помощью кодировщика\n",
        "                if key == \"name\":                     # если встретили имя\n",
        "                    num_tokens += tokens_per_name     # то добавили число токенов на\n",
        "        num_tokens += 3                               # каждый ответ оборачивается в <|start|>assistant<|message|>\n",
        "        return num_tokens                             # возвращаем число токенов\n",
        "\n",
        "\n",
        "    # Метод запроса к языковой модели\n",
        "    def answer_index(self, system, topic, temp = 1):\n",
        "        # Проверяем обучена ли наша модель\n",
        "        if not self.search_index:\n",
        "            self.log += 'Модель необходимо обучить! \\n'\n",
        "            return ''\n",
        "\n",
        "        # Выборка документов по схожести с запросом из векторной базы данных, topic- строка запроса, k - число извлекаемых фрагментов\n",
        "        docs = self.search_index.similarity_search(topic, k=5)\n",
        "        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        self.log += 'Выбираем документы по степени схожести с вопросом из векторной базы данных: \\n '\n",
        "        # Очищаем запрос от двойных пустых строк. Каждый фрагмент подписываем: Отрывок документа № и дальше порядковый номер\n",
        "        message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'Отрывок документа №{i+1}:\\n' + doc.page_content + '\\\\n' for i, doc in enumerate(docs)]))\n",
        "        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        self.log += f'{message_content} \\n'\n",
        "\n",
        "        # В системную роль помещаем найденные фрагменты и промпт, в пользовательскую - вопрос от пользователя\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system + f\"{message_content}\"},\n",
        "            {\"role\": \"user\", \"content\": topic}\n",
        "        ]\n",
        "\n",
        "        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        self.log += f\"\\n\\nТокенов использовано на вопрос по версии TikToken: {self.num_tokens_from_messages(messages, self.model)}\\n\"\n",
        "\n",
        "\n",
        "        # Запрос к языковой моделе\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=self.model,   # используемая модель\n",
        "            messages=messages,  # список форматированных сообщений с ролями\n",
        "            temperature=temp    # точность ответов модели\n",
        "        )\n",
        "\n",
        "\n",
        "        # Вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        self.log += '\\nСтатистика по токенам от языковой модели:\\n'\n",
        "        self.log += f'Токенов использовано всего (вопрос): {completion.usage.prompt_tokens} \\n'       # Число токенов на вопрос по расчетам LLM\n",
        "        self.log += f'Токенов использовано всего (вопрос-ответ): {completion.usage.total_tokens} \\n'  # Число токенов на вопрос и ответ по расчетам LLM\n",
        "\n",
        "        return completion.choices[0].message.content # возвращаем результат предсказания"
      ],
      "metadata": {
        "id": "GFYzCdDtfI3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Объявляем экземпляр класса GPT (созданный ранее) и передаем ему в конструктор модель LLM, с которой будем работать\n",
        "gpt = GPT(\"gpt-3.5-turbo\")\n",
        "\n",
        "# Gradio позволяет объединять элементы в блоки\n",
        "blocks = gr.Blocks()\n",
        "\n",
        "# Работаем с блоком\n",
        "with blocks as demo:\n",
        "    # Объявляем элемент выбор из списка (с подписью Данные), список выбирает из поля name нашей переменной models\n",
        "    subject = gr.Dropdown([(elem[\"name\"], index) for index, elem in enumerate(models)], label=\"Данные\")\n",
        "    # Здесь отобразиться выбранное имя name из списка\n",
        "    name = gr.Label(show_label=False)\n",
        "    # Промпт для запроса к LLM (по умолчанию поле prompt из models)\n",
        "    prompt = gr.Textbox(label=\"Промт\", interactive=True)\n",
        "    # Ссылка на файл обучения (по умолчанию поле doc из models)\n",
        "    link = gr.HTML()\n",
        "    # Поле пользовательского запроса к LLM (по умолчанию поле query из models)\n",
        "    query = gr.Textbox(label=\"Запрос к LLM\", interactive=True)\n",
        "\n",
        "\n",
        "    # Функция на выбор нейро-сотрудника в models\n",
        "    # Ей передается параметр subject - выбранное значение в поле списка\n",
        "    # А возвращаемые значения извлекаются из models\n",
        "    def onchange(dropdown):\n",
        "      return [\n",
        "          models[dropdown]['name'],                               # имя возвращается без изменения\n",
        "          re.sub('\\t+|\\s\\s+', ' ', models[dropdown]['prompt']),   # в промте удаляются двойные пробелы \\s\\s+ и табуляция \\t+\n",
        "          models[dropdown]['query'],                              # запрос возвращается без изменения\n",
        "          f\"<a target='_blank' href = '{models[dropdown]['doc']}'>Документ для обучения</a>\" # ссылка на документ оборачивается в html тег <a>  (https://htmlbook.ru/html/a)\n",
        "          ]\n",
        "\n",
        "    # При изменении значения в поле списка subject, вызывается функция onchange\n",
        "    # Ей передается параметр subject - выбранное значение в поле списка\n",
        "    # А возвращаемые значения устанавливаются в элементы name, prompt, query и link\n",
        "    subject.change(onchange, inputs = [subject], outputs = [name, prompt, query, link])\n",
        "\n",
        "    # Строку в gradio можно разделить на столбцы (каждая кнопка в своем столбце)\n",
        "    with gr.Row():\n",
        "        train_btn = gr.Button(\"Обучить модель\")       # кнопка запуска обучения\n",
        "        request_btn = gr.Button(\"Запрос к модели\")    # кнопка отправки запроса к LLM\n",
        "\n",
        "    # функция обучения\n",
        "    def train(dropdown):\n",
        "        # парсим документ и сохраняем его в базу данных\n",
        "        gpt.load_search_indexes(models[dropdown]['doc'])\n",
        "        return gpt.log\n",
        "\n",
        "    # Вызываем метод запроса к языковой модели из класса GPT\n",
        "    def predict(p, q):\n",
        "        #\n",
        "        result = gpt.answer_index(\n",
        "            p,\n",
        "            q\n",
        "        )\n",
        "        # возвращает список из ответа от LLM и log от класса GPT\n",
        "        return [result, gpt.log]\n",
        "\n",
        "    # Выводим поля response с ответом от LLM и log (вывод сообщений работы класса GPT) на 2 колонки\n",
        "    with gr.Row():\n",
        "        response = gr.Textbox(label=\"Ответ LLM\") # Текстовое поле с ответом от LLM\n",
        "        log = gr.Textbox(label=\"Логирование\")    # Текстовое поле с выводом сообщений от GPT\n",
        "\n",
        "\n",
        "    # При нажатии на кнопку train_btn запускается функция обучения train_btn с параметром subject\n",
        "    # Результат выполнения функции сохраняем в текстовое поле log - лог выполнения\n",
        "    train_btn.click(train, [subject], log)\n",
        "\n",
        "    # При нажатии на кнопку request_btn запускается функция отправки запроса к LLM request_btn с параметром prompt, query\n",
        "    # Результат выполнения функции сохраняем в текстовые поля  response - ответ модели, log - лог выполнения\n",
        "    request_btn.click(predict, [prompt, query], [response, log])\n",
        "\n",
        "# Запуск приложения\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "WnGEzYTcfNNT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}